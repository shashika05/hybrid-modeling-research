# -*- coding: utf-8 -*-
"""Ablation Studies - Model Architecture Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14qUpMrX10CFQi6lEUxV4Q2rItKX89eCM

#Cell 1: Imports and Environment Setup
"""

import os
import time
import warnings
import json
from tqdm.auto import tqdm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image

import timm
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    ConfusionMatrixDisplay
)

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette('husl')

try:
    torch.multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

print('✓ Imports complete')
print(f'PyTorch version: {torch.__version__}')
print(f'TIMM version: {timm.__version__}')

"""#Cell 2: Configuration and Device Setup"""

class Config:
    """Training configuration for ablation studies"""
    # Paths
    DATA_ROOT = '/content/drive/MyDrive/Research - Brain Cancer MRI/combined_brain_tumor_dataset'
    OUTPUT_DIR = '/content/drive/MyDrive/Research - Brain Cancer MRI/ablation_results'

    # Dataset settings
    TRAIN_DIR = os.path.join(DATA_ROOT, 'train')
    VAL_DIR = os.path.join(DATA_ROOT, 'val')
    TEST_DIR = os.path.join(DATA_ROOT, 'test')

    CLASS_NAMES = ['glioma', 'meningioma', 'no_tumor', 'pituitary']
    NUM_CLASSES = 4

    # Training hyperparameters
    NUM_EPOCHS = 12
    LEARNING_RATE = 3e-4
    WEIGHT_DECAY = 0.05
    IMG_SIZE = 224

    # Normalization (ImageNet stats)
    NORMALIZE_MEAN = [0.485, 0.456, 0.406]
    NORMALIZE_STD = [0.229, 0.224, 0.225]

    # Training settings
    NUM_WORKERS = 0
    PIN_MEMORY = True
    SEED = 42

    # Model-specific batch sizes (optimized for A100 GPU)
    BATCH_SIZES = {
        # Study 1: CNN Architectures
        'convnext_tiny': 512,
        'efficientnet_b3': 256,
        'resnet50': 384,
        'resnext50_32x4d': 384,

        # Study 2: Transformer Architectures
        'swin_tiny': 256,
        'vit_small_patch16_224': 256,
        'deit_small_patch16_224': 256,
        'twins_pcpvt_small': 256,

        # Study 3: Model Size Impact
        'mobilenetv3_large_100': 512,
        'vit_small_patch32_224': 384,
        'convnext_small': 256,
        'swin_small': 128,
    }

os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

def set_seed(seed=42):
    """Set random seeds for reproducibility"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

set_seed(Config.SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'\n{"="*60}')
print(f'Device: {device}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f'Total GPU Memory: {total_memory:.2f} GB')
print(f'{"="*60}\n')

"""# Cell 3: Custom Dataset Class"""

class BrainTumorDataset(Dataset):
    """Custom dataset for preprocessed brain tumor MRI images"""

    def __init__(self, data_dir, transform=None, class_names=None):
        self.data_dir = data_dir
        self.transform = transform
        self.class_names = class_names or Config.CLASS_NAMES
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.class_names)}
        self.samples = []
        self._load_samples()

    def _load_samples(self):
        """Load all image paths and labels"""
        for class_name in self.class_names:
            class_dir = os.path.join(self.data_dir, class_name)
            if not os.path.exists(class_dir):
                continue
            class_idx = self.class_to_idx[class_name]
            for img_file in os.listdir(class_dir):
                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(class_dir, img_file)
                    self.samples.append((img_path, class_idx))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception:
            image = Image.new('RGB', (Config.IMG_SIZE, Config.IMG_SIZE))

        if self.transform:
            image = self.transform(image)
        return image, label

"""# Cell 4: Data Transforms and DataLoaders"""

def get_transforms(split='train'):
    """Get data transforms for train/val/test splits"""
    if split == 'train':
        return transforms.Compose([
            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=Config.NORMALIZE_MEAN, std=Config.NORMALIZE_STD)
        ])
    else:
        return transforms.Compose([
            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=Config.NORMALIZE_MEAN, std=Config.NORMALIZE_STD)
        ])

def create_dataloaders(batch_size):
    """Create DataLoaders for train, validation, and test sets"""
    train_dataset = BrainTumorDataset(Config.TRAIN_DIR, transform=get_transforms('train'))
    val_dataset = BrainTumorDataset(Config.VAL_DIR, transform=get_transforms('val'))
    test_dataset = BrainTumorDataset(Config.TEST_DIR, transform=get_transforms('test'))

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                            num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                          num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                           num_workers=Config.NUM_WORKERS, pin_memory=Config.PIN_MEMORY)

    return train_loader, val_loader, test_loader

print('✓ Data utilities ready')

"""# Cell 5: Model Factory Functions"""

def create_model(model_name, num_classes=Config.NUM_CLASSES, pretrained=False):
    """
    Universal model creation function for all architectures

    Args:
        model_name: TIMM model name string
        num_classes: Number of output classes
        pretrained: Use pretrained weights

    Returns:
        PyTorch model
    """
    model = timm.create_model(
        model_name,
        pretrained=pretrained,
        num_classes=num_classes
    )
    return model

def count_parameters(model):
    """Count trainable parameters in model"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def get_model_info(model_name):
    """Get model information including parameter count"""
    model = create_model(model_name, pretrained=False)
    params = count_parameters(model)
    del model
    torch.cuda.empty_cache()
    return params

print('✓ Model factory ready')

"""# Cell 6: Loss Function"""

def focal_loss(inputs, targets, alpha=1.0, gamma=2.0, reduction='mean'):
    """
    Focal Loss for handling class imbalance
    FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)
    """
    ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
    pt = torch.exp(-ce_loss)
    focal_loss = alpha * (1 - pt) ** gamma * ce_loss

    if reduction == 'mean':
        return focal_loss.mean()
    elif reduction == 'sum':
        return focal_loss.sum()
    return focal_loss

print('✓ Focal loss ready')

"""# Cell 7: Training and Validation Functions"""

def train_one_epoch(model, loader, optimizer, scaler, device, epoch):
    """Train model for one epoch"""
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc=f'Epoch {epoch} [Train]')

    for batch_idx, (images, labels) in enumerate(pbar):
        images = images.to(device)
        labels = labels.long().to(device)

        optimizer.zero_grad()

        with autocast():
            outputs = model(images)
            loss = focal_loss(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        predictions = outputs.argmax(dim=1)
        correct += predictions.eq(labels).sum().item()
        total += labels.size(0)

        avg_loss = total_loss / (batch_idx + 1)
        accuracy = 100.0 * correct / total
        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'acc': f'{accuracy:.2f}%'})

    return avg_loss, accuracy

def validate_model(model, loader, device, phase='Val'):
    """Validate model on validation or test set"""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        pbar = tqdm(loader, desc=f'[{phase}]')

        for batch_idx, (images, labels) in enumerate(pbar):
            images = images.to(device)
            labels = labels.long().to(device)

            with autocast():
                outputs = model(images)
                loss = focal_loss(outputs, labels)

            total_loss += loss.item()
            probs = torch.softmax(outputs, dim=1)
            predictions = outputs.argmax(dim=1)
            correct += predictions.eq(labels).sum().item()
            total += labels.size(0)

            all_preds.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

            avg_loss = total_loss / (batch_idx + 1)
            accuracy = 100.0 * correct / total
            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'acc': f'{accuracy:.2f}%'})

    return avg_loss, accuracy, np.array(all_preds), np.array(all_labels), np.array(all_probs)

def measure_inference_time(model, device, input_size=(1, 3, 224, 224), runs=100):
    """Measure average inference time"""
    model.eval()
    dummy_input = torch.randn(input_size).to(device)

    with torch.no_grad():
        for _ in range(10):
            _ = model(dummy_input)

    if device.type == 'cuda':
        torch.cuda.synchronize()

    start_time = time.time()
    with torch.no_grad():
        for _ in range(runs):
            _ = model(dummy_input)

    if device.type == 'cuda':
        torch.cuda.synchronize()

    avg_time = (time.time() - start_time) / runs * 1000
    return avg_time

print('✓ Training utilities ready')

"""# Cell 8: Main Training Pipeline"""

def train_model(model_name, num_epochs=Config.NUM_EPOCHS, pretrained=False):
    """
    Complete training pipeline for a single model

    Args:
        model_name: TIMM model name
        num_epochs: Number of training epochs
        pretrained: Use pretrained weights

    Returns:
        Dictionary containing training history and metrics
    """
    print(f'\n{"="*60}')
    print(f'Training {model_name}')
    print(f'{"="*60}\n')

    # Get batch size
    batch_size = Config.BATCH_SIZES.get(model_name, 256)
    print(f'Batch size: {batch_size}')

    # Create DataLoaders
    train_loader, val_loader, test_loader = create_dataloaders(batch_size)

    # Create model
    model = create_model(model_name, pretrained=pretrained).to(device)
    num_params = count_parameters(model)
    print(f'Model parameters: {num_params / 1e6:.2f}M')

    # Setup optimizer and scheduler
    optimizer = optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE,
                           weight_decay=Config.WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    scaler = GradScaler()

    # Training history
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'learning_rates': []
    }

    best_val_acc = 0.0
    best_epoch = 0
    best_model_path = os.path.join(Config.OUTPUT_DIR, f'{model_name}_best.pth')

    # Training loop
    for epoch in range(1, num_epochs + 1):
        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler, device, epoch)
        val_loss, val_acc, _, _, _ = validate_model(model, val_loader, device, phase='Val')

        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step()

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['learning_rates'].append(current_lr)

        print(f'\nEpoch {epoch}/{num_epochs}:')
        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch
            torch.save(model.state_dict(), best_model_path)
            print(f'  ✓ Best model saved (Val Acc: {val_acc:.2f}%)')
        print()

    # Load best model for testing
    model.load_state_dict(torch.load(best_model_path))

    # Test evaluation
    test_loss, test_acc, test_preds, test_labels, test_probs = validate_model(
        model, test_loader, device, phase='Test')

    # Measure inference time
    inf_time = measure_inference_time(model, device)

    results = {
        'model_name': model_name,
        'history': history,
        'best_epoch': best_epoch,
        'best_val_acc': best_val_acc,
        'test_loss': test_loss,
        'test_acc': test_acc,
        'test_preds': test_preds,
        'test_labels': test_labels,
        'test_probs': test_probs,
        'num_params': num_params,
        'inference_time': inf_time
    }

    print(f'\n{"="*60}')
    print(f'{model_name} Complete')
    print(f'Best Val: {best_val_acc:.2f}% | Test: {test_acc:.2f}%')
    print(f'Params: {num_params / 1e6:.2f}M | Inference: {inf_time:.2f} ms')
    print(f'{"="*60}\n')

    del model
    torch.cuda.empty_cache()

    return results

print('✓ Training pipeline ready')

"""# Cell 9: Ablation Study Configurations"""

# Define all ablation studies
ABLATION_STUDIES = {
    'Study_1_CNN_Architectures': {
        'models': [
            'convnext_tiny',        # Baseline
            'efficientnet_b3',      # EfficientNet
            'resnet50',             # ResNet
            'resnext50_32x4d',      # ResNeXt
        ],
        'description': 'Comparing different CNN architectures'
    },

    'Study_2_Transformer_Architectures': {
        'models': [
            'swin_tiny_patch4_window7_224',  # Baseline
            'vit_small_patch16_224',         # Vision Transformer
            'deit_small_patch16_224',        # DeiT
            'twins_pcpvt_small',             # Twins
        ],
        'description': 'Comparing different Transformer architectures'
    },

    'Study_3_Model_Size_Impact': {
        'models': [
            # Small models
            ('mobilenetv3_large_100', 'CNN_Small'),
            ('vit_small_patch32_224', 'ViT_Small'),
            # Medium models
            ('convnext_tiny', 'CNN_Medium'),
            ('swin_tiny_patch4_window7_224', 'ViT_Medium'),
            # Large models
            ('convnext_small', 'CNN_Large'),
            ('swin_small_patch4_window7_224', 'ViT_Large'),
        ],
        'description': 'Analyzing impact of model size on performance'
    }
}

def print_study_info():
    """Print information about all ablation studies"""
    print(f'\n{"="*70}')
    print('ABLATION STUDIES OVERVIEW')
    print(f'{"="*70}\n')

    for study_name, study_config in ABLATION_STUDIES.items():
        print(f'{study_name}:')
        print(f'  {study_config["description"]}')
        print(f'  Models: {len(study_config["models"])}')

        if study_name == 'Study_3_Model_Size_Impact':
            for model_info in study_config['models']:
                model_name = model_info[0]
                params = get_model_info(model_name)
                print(f'    - {model_info[1]}: {params/1e6:.1f}M params')
        else:
            for model_name in study_config['models']:
                params = get_model_info(model_name)
                print(f'    - {model_name}: {params/1e6:.1f}M params')
        print()

print_study_info()

"""# Cell 10: Run Study 1 - CNN Architecture Comparison"""

def run_study_1():
    """Run CNN Architecture Comparison"""
    print(f'\n{"="*70}')
    print('STUDY 1: CNN ARCHITECTURE COMPARISON')
    print(f'{"="*70}\n')

    study_config = ABLATION_STUDIES['Study_1_CNN_Architectures']
    results = []

    for model_name in study_config['models']:
        result = train_model(model_name, num_epochs=Config.NUM_EPOCHS, pretrained=False)
        results.append(result)

    # Save results
    save_study_results(results, 'Study_1_CNN_Architectures')

    return results

# Uncomment to run
# study_1_results = run_study_1()

"""# Cell 11: Run Study 2 - Transformer Architecture Comparison"""

def run_study_2():
    """Run Transformer Architecture Comparison"""
    print(f'\n{"="*70}')
    print('STUDY 2: TRANSFORMER ARCHITECTURE COMPARISON')
    print(f'{"="*70}\n')

    study_config = ABLATION_STUDIES['Study_2_Transformer_Architectures']
    results = []

    for model_name in study_config['models']:
        result = train_model(model_name, num_epochs=Config.NUM_EPOCHS, pretrained=False)
        results.append(result)

    # Save results
    save_study_results(results, 'Study_2_Transformer_Architectures')

    return results

# Uncomment to run
# study_2_results = run_study_2()

"""# Cell 12: Run Study 3 - Model Size Impact Analysis"""

def run_study_3():
    """Run Model Size Impact Analysis"""
    print(f'\n{"="*70}')
    print('STUDY 3: MODEL SIZE IMPACT ANALYSIS')
    print(f'{"="*70}\n')

    study_config = ABLATION_STUDIES['Study_3_Model_Size_Impact']
    results = []

    for model_info in study_config['models']:
        model_name = model_info[0]
        display_name = model_info[1]

        result = train_model(model_name, num_epochs=Config.NUM_EPOCHS, pretrained=False)
        result['display_name'] = display_name
        results.append(result)

    # Save results
    save_study_results(results, 'Study_3_Model_Size_Impact')

    return results

# Uncomment to run
# study_3_results = run_study_3()

"""# Cell 13: Results Saving Utility"""

def save_study_results(results, study_name):
    """Save study results to JSON and CSV"""
    study_dir = os.path.join(Config.OUTPUT_DIR, study_name)
    os.makedirs(study_dir, exist_ok=True)

    # Summary DataFrame
    summary_rows = []
    for res in results:
        display_name = res.get('display_name', res['model_name'])
        labels = res['test_labels']
        preds = res['test_preds']

        precision = precision_score(labels, preds, average='macro') * 100
        recall = recall_score(labels, preds, average='macro') * 100
        f1 = f1_score(labels, preds, average='macro') * 100

        summary_rows.append({
            'Model': display_name,
            'Parameters (M)': res['num_params'] / 1e6,
            'Inference (ms)': res['inference_time'],
            'Best Val Acc (%)': res['best_val_acc'],
            'Test Acc (%)': res['test_acc'],
            'Precision (%)': precision,
            'Recall (%)': recall,
            'F1-Score (%)': f1
        })

    summary_df = pd.DataFrame(summary_rows)
    csv_path = os.path.join(study_dir, 'summary.csv')
    summary_df.to_csv(csv_path, index=False)

    # JSON results
    json_results = []
    for res in results:
        json_res = {
            'model_name': res['model_name'],
            'display_name': res.get('display_name', res['model_name']),
            'best_epoch': int(res['best_epoch']),
            'best_val_acc': float(res['best_val_acc']),
            'test_acc': float(res['test_acc']),
            'num_params': int(res['num_params']),
            'inference_time': float(res['inference_time']),
            'history': {k: [float(x) for x in v] for k, v in res['history'].items()}
        }
        json_results.append(json_res)

    json_path = os.path.join(study_dir, 'results.json')
    with open(json_path, 'w') as f:
        json.dump(json_results, f, indent=4)

    print(f'✓ Results saved to {study_dir}')
    return summary_df

print('✓ Results saving utility ready')

"""# Cell 14: Visualization Functions"""

def visualize_study_results(results, study_name):
    """Generate comprehensive visualizations for a study"""
    study_dir = os.path.join(Config.OUTPUT_DIR, study_name)
    os.makedirs(study_dir, exist_ok=True)

    # Extract model names
    model_names = [res.get('display_name', res['model_name']) for res in results]

    # 1. Training Curves
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    for res, name in zip(results, model_names):
        history = res['history']
        epochs = range(1, len(history['train_acc']) + 1)
        axes[0].plot(epochs, history['train_acc'], label=f'{name} Train', linestyle='-')
        axes[0].plot(epochs, history['val_acc'], label=f'{name} Val', linestyle='--')

    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Accuracy (%)', fontsize=12)
    axes[0].set_title('Training Curves', fontsize=14)
    axes[0].legend(fontsize=10)
    axes[0].grid(alpha=0.3)

    # Loss curves
    for res, name in zip(results, model_names):
        history = res['history']
        epochs = range(1, len(history['train_loss']) + 1)
        axes[1].plot(epochs, history['train_loss'], label=f'{name} Train', linestyle='-')
        axes[1].plot(epochs, history['val_loss'], label=f'{name} Val', linestyle='--')

    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Loss', fontsize=12)
    axes[1].set_title('Loss Curves', fontsize=14)
    axes[1].legend(fontsize=10)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(study_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')
    plt.show()

    # 2. Performance Comparison
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    metrics_data = {
        'Test Accuracy (%)': [res['test_acc'] for res in results],
        'Parameters (M)': [res['num_params']/1e6 for res in results],
        'Inference Time (ms)': [res['inference_time'] for res in results],
        'Best Val Acc (%)': [res['best_val_acc'] for res in results]
    }

    for ax, (metric, values) in zip(axes, metrics_data.items()):
        bars = ax.bar(model_names, values)
        ax.set_ylabel(metric, fontsize=12)
        ax.set_title(metric, fontsize=14)
        ax.tick_params(axis='x', rotation=45)

        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.2f}', ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.savefig(os.path.join(study_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()

    # 3. Confusion Matrices
    n_models = len(results)
    fig, axes = plt.subplots(n_models, 1, figsize=(8, 5*n_models))
    if n_models == 1:
        axes = [axes]

    for ax, res, name in zip(axes, results, model_names):
        cm = confusion_matrix(res['test_labels'], res['test_preds'])
        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

        sns.heatmap(cm_norm, annot=True, fmt='.1f', cmap='Blues',
                   xticklabels=Config.CLASS_NAMES,
                   yticklabels=Config.CLASS_NAMES,
                   ax=ax, cbar_kws={'label': 'Percentage (%)'})
        ax.set_title(f'{name} Confusion Matrix', fontsize=14)
        ax.set_ylabel('True Label', fontsize=12)
        ax.set_xlabel('Predicted Label', fontsize=12)

    plt.tight_layout()
    plt.savefig(os.path.join(study_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')
    plt.show()

    # 4. Efficiency Analysis
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    params = [res['num_params']/1e6 for res in results]
    accuracy = [res['test_acc'] for res in results]
    inf_time = [res['inference_time'] for res in results]

    # Parameters vs Accuracy
    for i, name in enumerate(model_names):
        ax1.scatter(params[i], accuracy[i], s=200, label=name, alpha=0.7)
    ax1.set_xlabel('Parameters (M)', fontsize=12)
    ax1.set_ylabel('Test Accuracy (%)', fontsize=12)
    ax1.set_title('Model Size vs Accuracy', fontsize=14)
    ax1.legend(fontsize=10)
    ax1.grid(alpha=0.3)

    # Inference Time vs Accuracy
    for i, name in enumerate(model_names):
        ax2.scatter(inf_time[i], accuracy[i], s=200, label=name, alpha=0.7)
    ax2.set_xlabel('Inference Time (ms)', fontsize=12)
    ax2.set_ylabel('Test Accuracy (%)', fontsize=12)
    ax2.set_title('Speed vs Accuracy', fontsize=14)
    ax2.legend(fontsize=10)
    ax2.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(study_dir, 'efficiency_analysis.png'), dpi=300, bbox_inches='tight')
    plt.show()

    print(f'✓ Visualizations saved to {study_dir}')

print('✓ Visualization functions ready')

"""# Cell 15: Compare All Studies"""

def compare_all_studies(study_1_results=None, study_2_results=None, study_3_results=None):
    """Compare results across all ablation studies"""
    print(f'\n{"="*70}')
    print('CROSS-STUDY COMPARISON')
    print(f'{"="*70}\n')

    all_results = []
    study_labels = []

    if study_1_results:
        all_results.extend(study_1_results)
        study_labels.extend(['CNN'] * len(study_1_results))

    if study_2_results:
        all_results.extend(study_2_results)
        study_labels.extend(['Transformer'] * len(study_2_results))

    if study_3_results:
        all_results.extend(study_3_results)
        study_labels.extend(['Size'] * len(study_3_results))

    if not all_results:
        print('No results available. Run studies first.')
        return

    # Create comprehensive comparison
    comparison_data = []
    for res, study in zip(all_results, study_labels):
        display_name = res.get('display_name', res['model_name'])
        labels = res['test_labels']
        preds = res['test_preds']

        precision = precision_score(labels, preds, average='macro') * 100
        recall = recall_score(labels, preds, average='macro') * 100
        f1 = f1_score(labels, preds, average='macro') * 100

        comparison_data.append({
            'Study': study,
            'Model': display_name,
            'Parameters (M)': res['num_params'] / 1e6,
            'Inference (ms)': res['inference_time'],
            'Test Acc (%)': res['test_acc'],
            'Precision (%)': precision,
            'Recall (%)': recall,
            'F1-Score (%)': f1
        })

    df = pd.DataFrame(comparison_data)

    # Print comparison table
    print('\nOverall Performance Comparison:')
    print(df.to_string(index=False))
    print()

    # Save comparison
    comparison_path = os.path.join(Config.OUTPUT_DIR, 'all_studies_comparison.csv')
    df.to_csv(comparison_path, index=False)
    print(f'✓ Comparison saved to {comparison_path}')

    # Visualization: Accuracy vs Efficiency
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Group by study
    for study in df['Study'].unique():
        study_df = df[df['Study'] == study]

        # Accuracy comparison
        axes[0].scatter(study_df['Parameters (M)'], study_df['Test Acc (%)'],
                       s=150, label=study, alpha=0.7)

        # Inference time comparison
        axes[1].scatter(study_df['Inference (ms)'], study_df['Test Acc (%)'],
                       s=150, label=study, alpha=0.7)

        # F1 Score comparison
        axes[2].scatter(study_df['Parameters (M)'], study_df['F1-Score (%)'],
                       s=150, label=study, alpha=0.7)

    axes[0].set_xlabel('Parameters (M)', fontsize=12)
    axes[0].set_ylabel('Test Accuracy (%)', fontsize=12)
    axes[0].set_title('Model Size vs Accuracy', fontsize=14)
    axes[0].legend()
    axes[0].grid(alpha=0.3)

    axes[1].set_xlabel('Inference Time (ms)', fontsize=12)
    axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)
    axes[1].set_title('Speed vs Accuracy', fontsize=14)
    axes[1].legend()
    axes[1].grid(alpha=0.3)

    axes[2].set_xlabel('Parameters (M)', fontsize=12)
    axes[2].set_ylabel('F1-Score (%)', fontsize=12)
    axes[2].set_title('Model Size vs F1-Score', fontsize=14)
    axes[2].legend()
    axes[2].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(Config.OUTPUT_DIR, 'cross_study_comparison.png'),
                dpi=300, bbox_inches='tight')
    plt.show()

    # Find best models
    print('\n' + '='*70)
    print('BEST MODELS BY CATEGORY')
    print('='*70)

    best_accuracy = df.loc[df['Test Acc (%)'].idxmax()]
    print(f'\nHighest Accuracy:')
    print(f'  Model: {best_accuracy["Model"]}')
    print(f'  Test Acc: {best_accuracy["Test Acc (%)"]:.2f}%')
    print(f'  Parameters: {best_accuracy["Parameters (M)"]:.2f}M')

    best_speed = df.loc[df['Inference (ms)'].idxmin()]
    print(f'\nFastest Inference:')
    print(f'  Model: {best_speed["Model"]}')
    print(f'  Inference: {best_speed["Inference (ms)"]:.2f} ms')
    print(f'  Test Acc: {best_speed["Test Acc (%)"]:.2f}%')

    # Best trade-off (accuracy per parameter)
    df['Efficiency Score'] = df['Test Acc (%)'] / df['Parameters (M)']
    best_efficient = df.loc[df['Efficiency Score'].idxmax()]
    print(f'\nMost Parameter-Efficient:')
    print(f'  Model: {best_efficient["Model"]}')
    print(f'  Test Acc: {best_efficient["Test Acc (%)"]:.2f}%')
    print(f'  Parameters: {best_efficient["Parameters (M)"]:.2f}M')
    print(f'  Efficiency: {best_efficient["Efficiency Score"]:.2f} acc/M params')

    print('\n' + '='*70)

    return df

print('✓ Cross-study comparison ready')

"""# Cell 16: Run All Studies Sequentially"""

def run_all_studies():
    """Run all three ablation studies sequentially"""
    print(f'\n{"="*70}')
    print('RUNNING ALL ABLATION STUDIES')
    print(f'{"="*70}\n')

    # Study 1: CNN Architectures
    print('\n[1/3] Running CNN Architecture Comparison...')
    study_1_results = run_study_1()
    visualize_study_results(study_1_results, 'Study_1_CNN_Architectures')

    # Study 2: Transformer Architectures
    print('\n[2/3] Running Transformer Architecture Comparison...')
    study_2_results = run_study_2()
    visualize_study_results(study_2_results, 'Study_2_Transformer_Architectures')

    # Study 3: Model Size Impact
    print('\n[3/3] Running Model Size Impact Analysis...')
    study_3_results = run_study_3()
    visualize_study_results(study_3_results, 'Study_3_Model_Size_Impact')

    # Compare all studies
    print('\nGenerating cross-study comparison...')
    comparison_df = compare_all_studies(study_1_results, study_2_results, study_3_results)

    print(f'\n{"="*70}')
    print('ALL ABLATION STUDIES COMPLETE')
    print(f'{"="*70}')
    print(f'Results saved to: {Config.OUTPUT_DIR}')
    print('='*70)

    return {
        'study_1': study_1_results,
        'study_2': study_2_results,
        'study_3': study_3_results,
        'comparison': comparison_df
    }

print('✓ Complete pipeline ready')

"""# Cell 17: Generate Detailed Analysis Report"""

def generate_analysis_report(all_study_results):
    """Generate a detailed analysis report of all studies"""
    report_path = os.path.join(Config.OUTPUT_DIR, 'analysis_report.txt')

    with open(report_path, 'w') as f:
        f.write('='*70 + '\n')
        f.write('ABLATION STUDIES - COMPREHENSIVE ANALYSIS REPORT\n')
        f.write('='*70 + '\n\n')

        # Study 1 Analysis
        f.write('STUDY 1: CNN ARCHITECTURE COMPARISON\n')
        f.write('-'*70 + '\n')
        if 'study_1' in all_study_results:
            for res in all_study_results['study_1']:
                f.write(f"\nModel: {res['model_name']}\n")
                f.write(f"  Parameters: {res['num_params']/1e6:.2f}M\n")
                f.write(f"  Best Val Acc: {res['best_val_acc']:.2f}%\n")
                f.write(f"  Test Acc: {res['test_acc']:.2f}%\n")
                f.write(f"  Inference Time: {res['inference_time']:.2f} ms\n")

                # Per-class performance
                labels = res['test_labels']
                preds = res['test_preds']
                report = classification_report(labels, preds,
                                              target_names=Config.CLASS_NAMES,
                                              digits=4)
                f.write(f"\n{report}\n")

        f.write('\n' + '='*70 + '\n\n')

        # Study 2 Analysis
        f.write('STUDY 2: TRANSFORMER ARCHITECTURE COMPARISON\n')
        f.write('-'*70 + '\n')
        if 'study_2' in all_study_results:
            for res in all_study_results['study_2']:
                f.write(f"\nModel: {res['model_name']}\n")
                f.write(f"  Parameters: {res['num_params']/1e6:.2f}M\n")
                f.write(f"  Best Val Acc: {res['best_val_acc']:.2f}%\n")
                f.write(f"  Test Acc: {res['test_acc']:.2f}%\n")
                f.write(f"  Inference Time: {res['inference_time']:.2f} ms\n")

                labels = res['test_labels']
                preds = res['test_preds']
                report = classification_report(labels, preds,
                                              target_names=Config.CLASS_NAMES,
                                              digits=4)
                f.write(f"\n{report}\n")

        f.write('\n' + '='*70 + '\n\n')

        # Study 3 Analysis
        f.write('STUDY 3: MODEL SIZE IMPACT ANALYSIS\n')
        f.write('-'*70 + '\n')
        if 'study_3' in all_study_results:
            for res in all_study_results['study_3']:
                display_name = res.get('display_name', res['model_name'])
                f.write(f"\nModel: {display_name}\n")
                f.write(f"  Parameters: {res['num_params']/1e6:.2f}M\n")
                f.write(f"  Best Val Acc: {res['best_val_acc']:.2f}%\n")
                f.write(f"  Test Acc: {res['test_acc']:.2f}%\n")
                f.write(f"  Inference Time: {res['inference_time']:.2f} ms\n")

        f.write('\n' + '='*70 + '\n\n')

        # Key Findings
        f.write('KEY FINDINGS AND RECOMMENDATIONS\n')
        f.write('-'*70 + '\n\n')

        if 'comparison' in all_study_results and all_study_results['comparison'] is not None:
            df = all_study_results['comparison']

            best_acc = df.loc[df['Test Acc (%)'].idxmax()]
            f.write(f"Best Overall Accuracy:\n")
            f.write(f"  {best_acc['Model']}: {best_acc['Test Acc (%)']:.2f}%\n\n")

            fastest = df.loc[df['Inference (ms)'].idxmin()]
            f.write(f"Fastest Inference:\n")
            f.write(f"  {fastest['Model']}: {fastest['Inference (ms)']:.2f} ms\n\n")

            df['Efficiency'] = df['Test Acc (%)'] / df['Parameters (M)']
            best_eff = df.loc[df['Efficiency'].idxmax()]
            f.write(f"Most Efficient (Acc/Params):\n")
            f.write(f"  {best_eff['Model']}: {best_eff['Efficiency']:.2f}\n\n")

        f.write('='*70 + '\n')
        f.write('Report generated successfully\n')
        f.write('='*70 + '\n')

    print(f'✓ Analysis report saved to: {report_path}')

print('✓ Report generation ready')

"""# Cell 18: Execute Complete Ablation Study Pipeline"""

# UNCOMMENT TO RUN ALL STUDIES
print('\nStarting complete ablation study pipeline...')
all_results = run_all_studies()
generate_analysis_report(all_results)

# OR RUN INDIVIDUAL STUDIES:

# Run only Study 1
# study_1_results = run_study_1()
# visualize_study_results(study_1_results, 'Study_1_CNN_Architectures')

# Run only Study 2
# study_2_results = run_study_2()
# visualize_study_results(study_2_results, 'Study_2_Transformer_Architectures')

# Run only Study 3
# study_3_results = run_study_3()
# visualize_study_results(study_3_results, 'Study_3_Model_Size_Impact')

print('\n' + '='*70)
print('ABLATION STUDIES NOTEBOOK READY')
print('='*70)
print('\nTo execute:')
print('  1. Run all studies: Uncomment "all_results = run_all_studies()"')
print('  2. Run individual studies: Uncomment the specific study function')
print('  3. Compare results: Use compare_all_studies()')
print('  4. Generate report: Use generate_analysis_report()')
print('='*70)