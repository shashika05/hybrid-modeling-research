# -*- coding: utf-8 -*-
"""Ablation Studies - 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZkhBmoP36EIKwbgyc0Mu1RNO5ZBriYy-

#Brain Tumor Classification - Training Notebook
#Ablation Studies 2 ( Pretrained - False, 16 epochs)
This notebook trains ConvNeXt, Swin Transformer, and Hybrid models on preprocessed brain tumor MRI data.
Adapted to work with the combined_brain_tumor_dataset structure from preprocessing pipeline.

Dataset structure:


```
combined_brain_tumor_dataset/
├── train/
│   ├── glioma/
│   ├── meningioma/
│   ├── no_tumor/
│   └── pituitary/
├── val/
│   ├── glioma/
│   ├── meningioma/
│   ├── no_tumor/
│   └── pituitary/
└── test/
    ├── glioma/
    ├── meningioma/
    ├── no_tumor/
    └── pituitary/
```

# Cell 1: Imports and Environment Setup
"""

import os
import time
import warnings
import json
from tqdm.auto import tqdm

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image

import timm
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    ConfusionMatrixDisplay
)

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette('husl')

# Fix multiprocessing for Colab/Kaggle notebooks
try:
    torch.multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass

print('✓ Imports complete')
print(f'PyTorch version: {torch.__version__}')
print(f'TIMM version: {timm.__version__}')

"""# Cell 2: Configuration and Device Setup"""

class Config:
    """Training configuration"""
    # Paths from google drive
    DATA_ROOT = '/content/drive/MyDrive/Research - Brain Cancer MRI/combined_brain_tumor_dataset'
    OUTPUT_DIR = '/content/drive/MyDrive/Research - Brain Cancer MRI/training_results'

    # Dataset settings
    TRAIN_DIR = os.path.join(DATA_ROOT, 'train')
    VAL_DIR = os.path.join(DATA_ROOT, 'val')
    TEST_DIR = os.path.join(DATA_ROOT, 'test')

    # Class names
    CLASS_NAMES = ['glioma', 'meningioma', 'no_tumor', 'pituitary']
    NUM_CLASSES = 4

    # Training hyperparameters
    BATCH_SIZE = 32  # Will be adjusted per model based on GPU memory
    NUM_EPOCHS = 16
    LEARNING_RATE = 3e-4
    WEIGHT_DECAY = 0.05

    # Image resolution
    IMG_SIZE = 224

    # Normalization stats from preprocessing
    # ImageNet defaults as a common practice
    NORMALIZE_MEAN = [0.485, 0.456, 0.406]
    NORMALIZE_STD = [0.229, 0.224, 0.225]

    # Training settings
    NUM_WORKERS = 0  # Use 0 for Colab to avoid multiprocessing issues
    PIN_MEMORY = True
    SEED = 42

    # Model-specific batch sizes
    BATCH_SIZES = {
        'ConvNeXt': 512,
        'Swin': 256,
        'Hybrid': 128
    }
    # These values used here because of A100 GPU

# Create output directory
os.makedirs(Config.OUTPUT_DIR, exist_ok=True)

# Set random seeds for reproducibility
def set_seed(seed=42):
    """Set random seeds for reproducibility"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

set_seed(Config.SEED)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'\n{"="*60}')
print(f'Device: {device}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f'Total GPU Memory: {total_memory:.2f} GB')
    free_memory, total = torch.cuda.mem_get_info()
    print(f'Free GPU Memory: {free_memory / 1e9:.2f} GB')
print(f'{"="*60}\n')

"""# Cell 3: Custom Dataset Class for Preprocessed Data"""

class BrainTumorDataset(Dataset):
    """
    Custom dataset for preprocessed brain tumor MRI images.
    Loads JPG images from the preprocessed dataset structure.
    """

    def __init__(self, data_dir, transform=None, class_names=None):
        """
        Args:
            data_dir: Path to dataset split directory (train/val/test)
            transform: PyTorch transforms to apply
            class_names: List of class names (for consistency)
        """
        self.data_dir = data_dir
        self.transform = transform
        self.class_names = class_names or Config.CLASS_NAMES
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.class_names)}

        # Collect all image paths and labels
        self.samples = []
        self._load_samples()

        print(f'Loaded {len(self.samples)} samples from {data_dir}')
        self._print_class_distribution()

    def _load_samples(self):
        """Load all image paths and their corresponding labels"""
        for class_name in self.class_names:
            class_dir = os.path.join(self.data_dir, class_name)

            if not os.path.exists(class_dir):
                print(f'Warning: Class directory not found: {class_dir}')
                continue

            class_idx = self.class_to_idx[class_name]

            # Get all image files (JPG from preprocessing)
            for img_file in os.listdir(class_dir):
                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    img_path = os.path.join(class_dir, img_file)
                    self.samples.append((img_path, class_idx))

    def _print_class_distribution(self):
        """Print class distribution for this split"""
        class_counts = {cls: 0 for cls in self.class_names}
        for _, label in self.samples:
            class_name = self.class_names[label]
            class_counts[class_name] += 1

        print('  Class distribution:')
        for cls, count in class_counts.items():
            percentage = (count / len(self.samples)) * 100
            print(f'    {cls}: {count} ({percentage:.2f}%)')

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Returns:
            image: Transformed image tensor [3, 224, 224]
            label: Class index (0-3)
        """
        img_path, label = self.samples[idx]

        # Load image
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f'Error loading {img_path}: {e}')
            # Return a black image if loading fails
            image = Image.new('RGB', (Config.IMG_SIZE, Config.IMG_SIZE))

        # Apply transforms
        if self.transform:
            image = self.transform(image)

        return image, label

"""# Cell 4: Data Transforms"""

def get_transforms(split='train'):
    """
    Get data transforms for train/val/test splits.

    Args:
        split: 'train', 'val', or 'test'

    Returns:
        torchvision.transforms.Compose object
    """
    if split == 'train':
        # Training augmentations
        return transforms.Compose([
            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=Config.NORMALIZE_MEAN,
                               std=Config.NORMALIZE_STD)
        ])
    else:
        # Validation/Test - no augmentation
        return transforms.Compose([
            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(mean=Config.NORMALIZE_MEAN,
                               std=Config.NORMALIZE_STD)
        ])

print('✓ Transforms defined')

"""# Cell 5: Create DataLoaders"""

def create_dataloaders(batch_size=Config.BATCH_SIZE):
    """
    Create DataLoaders for train, validation, and test sets.

    Args:
        batch_size: Batch size for training

    Returns:
        train_loader, val_loader, test_loader
    """
    print(f'\n{"="*60}')
    print('Creating DataLoaders...')
    print(f'{"="*60}\n')

    # Create datasets
    train_dataset = BrainTumorDataset(
        Config.TRAIN_DIR,
        transform=get_transforms('train'),
        class_names=Config.CLASS_NAMES
    )

    val_dataset = BrainTumorDataset(
        Config.VAL_DIR,
        transform=get_transforms('val'),
        class_names=Config.CLASS_NAMES
    )

    test_dataset = BrainTumorDataset(
        Config.TEST_DIR,
        transform=get_transforms('test'),
        class_names=Config.CLASS_NAMES
    )

    # Create DataLoaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=Config.NUM_WORKERS,
        pin_memory=Config.PIN_MEMORY,
        drop_last=True  # Drop last incomplete batch
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=Config.NUM_WORKERS,
        pin_memory=Config.PIN_MEMORY
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=Config.NUM_WORKERS,
        pin_memory=Config.PIN_MEMORY
    )

    print(f'\n✓ DataLoaders created')
    print(f'  Train batches: {len(train_loader)}')
    print(f'  Val batches: {len(val_loader)}')
    print(f'  Test batches: {len(test_loader)}')

    return train_loader, val_loader, test_loader

"""# Cell 6: Model Architectures"""

def create_convnext(num_classes=Config.NUM_CLASSES, pretrained=False):
    """
    Create ConvNeXt-Tiny model.

    ConvNeXt is a pure CNN architecture that modernizes ResNet design.
    - Uses depthwise convolutions and inverted bottlenecks
    - Efficient and accurate
    - ~28M parameters
    """
    model = timm.create_model(
        'convnext_tiny',
        pretrained=pretrained,
        num_classes=num_classes
    )
    return model


def create_swin(num_classes=Config.NUM_CLASSES, pretrained=False):
    """
    Create Swin Transformer-Tiny model.

    Swin Transformer uses shifted windows for efficient self-attention.
    - Hierarchical architecture like CNNs
    - Better than standard ViT for vision tasks
    - ~28M parameters
    """
    model = timm.create_model(
        'swin_tiny_patch4_window7_224',
        pretrained=pretrained,
        num_classes=num_classes
    )
    return model


def create_hybrid(num_classes=Config.NUM_CLASSES, pretrained=False):
    """
    Create Hybrid CNN-Transformer model.

    Combines ConvNeXt and Swin Transformer features:
    - ConvNeXt extracts local spatial features
    - Swin Transformer captures global context
    - Fusion head combines both representations
    - ~56M parameters (sum of both backbones)
    """
    # Create feature extractors (no classification head)
    conv_backbone = timm.create_model(
        'convnext_tiny',
        pretrained=pretrained,
        num_classes=0  # Remove classification head
    )

    swin_backbone = timm.create_model(
        'swin_tiny_patch4_window7_224',
        pretrained=pretrained,
        num_classes=0  # Remove classification head
    )

    # Get feature dimensions
    conv_features = conv_backbone.num_features
    swin_features = swin_backbone.num_features

    # Create fusion head
    fusion_head = nn.Sequential(
        nn.Linear(conv_features + swin_features, 512),
        nn.BatchNorm1d(512),
        nn.ReLU(inplace=True),
        nn.Dropout(0.3),
        nn.Linear(512, 256),
        nn.Linear(256, num_classes)
    )

    # Wrapper model
    class HybridModel(nn.Module):
        def __init__(self, conv_backbone, swin_backbone, fusion_head):
            super().__init__()
            self.conv_backbone = conv_backbone
            self.swin_backbone = swin_backbone
            self.fusion_head = fusion_head
            self.pool = nn.AdaptiveAvgPool2d(1)

        def forward(self, x):
            # Extract CNN features
            conv_feat = self.conv_backbone(x)
            if conv_feat.ndim == 4:  # If output is [B, C, H, W]
                conv_feat = self.pool(conv_feat).flatten(1)

            # Extract Transformer features
            swin_feat = self.swin_backbone(x)
            if swin_feat.ndim == 4:  # If output is [B, C, H, W]
                swin_feat = self.pool(swin_feat).flatten(1)

            # Concatenate and classify
            combined = torch.cat([conv_feat, swin_feat], dim=1)
            output = self.fusion_head(combined)

            return output

    return HybridModel(conv_backbone, swin_backbone, fusion_head)


def count_parameters(model):
    """Count trainable parameters in model"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print('✓ Model factories ready')

"""# Cell 7: Focal Loss (for class imbalance handling)"""

def focal_loss(inputs, targets, alpha=1.0, gamma=2.0, reduction='mean'):
    """
    Focal Loss: Addresses class imbalance by down-weighting easy examples.

    FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)

    Args:
        inputs: Model predictions (logits) [batch_size, num_classes]
        targets: Ground truth labels [batch_size]
        alpha: Weighting factor (default 1.0)
        gamma: Focusing parameter (default 2.0)
        reduction: 'mean', 'sum', or 'none'

    Returns:
        Focal loss value
    """
    ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
    pt = torch.exp(-ce_loss)  # Probability of correct class
    focal_loss = alpha * (1 - pt) ** gamma * ce_loss

    if reduction == 'mean':
        return focal_loss.mean()
    elif reduction == 'sum':
        return focal_loss.sum()
    return focal_loss


print('✓ Focal loss ready')

"""# Cell 8: Training and Validation Functions"""

def train_one_epoch(model, loader, optimizer, scaler, device, epoch):
    """
    Train model for one epoch.

    Args:
        model: Neural network model
        loader: Training DataLoader
        optimizer: Optimizer (AdamW)
        scaler: Gradient scaler for mixed precision
        device: torch device
        epoch: Current epoch number

    Returns:
        avg_loss: Average training loss
        accuracy: Training accuracy (%)
    """
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc=f'Epoch {epoch} [Train]')

    for batch_idx, (images, labels) in enumerate(pbar):
        images = images.to(device)
        labels = labels.long().to(device)

        # Zero gradients
        optimizer.zero_grad()

        # Mixed precision forward pass
        with autocast():
            outputs = model(images)
            loss = focal_loss(outputs, labels)

        # Backward pass with gradient scaling
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        # Calculate metrics
        total_loss += loss.item()
        predictions = outputs.argmax(dim=1)
        correct += predictions.eq(labels).sum().item()
        total += labels.size(0)

        # Update progress bar
        avg_loss = total_loss / (batch_idx + 1)
        accuracy = 100.0 * correct / total
        pbar.set_postfix({
            'loss': f'{avg_loss:.4f}',
            'acc': f'{accuracy:.2f}%'
        })

    return avg_loss, accuracy


def validate_model(model, loader, device, phase='Val'):
    """
    Validate model on validation or test set.

    Args:
        model: Neural network model
        loader: Validation/Test DataLoader
        device: torch device
        phase: 'Val' or 'Test' (for display)

    Returns:
        avg_loss: Average validation loss
        accuracy: Validation accuracy (%)
        all_preds: All predictions
        all_labels: All ground truth labels
        all_probs: All class probabilities
    """
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0

    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        pbar = tqdm(loader, desc=f'[{phase}]')

        for batch_idx, (images, labels) in enumerate(pbar):
            images = images.to(device)
            labels = labels.long().to(device)

            # Mixed precision inference
            with autocast():
                outputs = model(images)
                loss = focal_loss(outputs, labels)

            # Calculate metrics
            total_loss += loss.item()
            probs = torch.softmax(outputs, dim=1)
            predictions = outputs.argmax(dim=1)
            correct += predictions.eq(labels).sum().item()
            total += labels.size(0)

            # Store predictions for detailed metrics
            all_preds.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

            # Update progress bar
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = 100.0 * correct / total
            pbar.set_postfix({
                'loss': f'{avg_loss:.4f}',
                'acc': f'{accuracy:.2f}%'
            })

    return (
        avg_loss,
        accuracy,
        np.array(all_preds),
        np.array(all_labels),
        np.array(all_probs)
    )


def measure_inference_time(model, device, input_size=(1, 3, 224, 224), runs=100):
    """
    Measure average inference time.

    Args:
        model: Neural network model
        device: torch device
        input_size: Input tensor size
        runs: Number of inference runs

    Returns:
        Average inference time in milliseconds
    """
    model.eval()
    dummy_input = torch.randn(input_size).to(device)

    # Warmup
    with torch.no_grad():
        for _ in range(10):
            _ = model(dummy_input)

    # Measure
    if device.type == 'cuda':
        torch.cuda.synchronize()

    start_time = time.time()
    with torch.no_grad():
        for _ in range(runs):
            _ = model(dummy_input)

    if device.type == 'cuda':
        torch.cuda.synchronize()

    avg_time = (time.time() - start_time) / runs * 1000  # Convert to ms
    return avg_time


print('✓ Training utilities ready')

"""# Cell 9: Main Training Loop"""

def train_model(model_factory, model_name, num_epochs=Config.NUM_EPOCHS):
    """
    Complete training pipeline for a single model.

    Args:
        model_factory: Function that creates the model
        model_name: Name of the model ('ConvNeXt', 'Swin', 'Hybrid')
        num_epochs: Number of training epochs

    Returns:
        Dictionary containing training history and evaluation metrics
    """
    print(f'\n{"="*60}')
    print(f'Training {model_name}')
    print(f'{"="*60}\n')

    # Get model-specific batch size
    batch_size = Config.BATCH_SIZES.get(model_name, Config.BATCH_SIZE)
    print(f'Batch size: {batch_size}')

    # Create DataLoaders
    train_loader, val_loader, test_loader = create_dataloaders(batch_size)

    # Create model
    model = model_factory().to(device)
    num_params = count_parameters(model)
    print(f'Model parameters: {num_params / 1e6:.2f}M')

    # Setup optimizer and scheduler
    optimizer = optim.AdamW(
        model.parameters(),
        lr=Config.LEARNING_RATE,
        weight_decay=Config.WEIGHT_DECAY
    )

    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs
    )

    # Gradient scaler for mixed precision
    scaler = GradScaler()

    # Training history
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'learning_rates': []
    }

    # Track best model
    best_val_acc = 0.0
    best_epoch = 0
    best_model_path = os.path.join(Config.OUTPUT_DIR, f'{model_name}_best.pth')

    # Training loop
    print(f'\nStarting training for {num_epochs} epochs...\n')

    for epoch in range(1, num_epochs + 1):
        # Train
        train_loss, train_acc = train_one_epoch(
            model, train_loader, optimizer, scaler, device, epoch
        )

        # Validate
        val_loss, val_acc, _, _, _ = validate_model(
            model, val_loader, device, phase='Val'
        )

        # Update learning rate
        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step()

        # Save history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['learning_rates'].append(current_lr)

        # Print epoch summary
        print(f'\nEpoch {epoch}/{num_epochs} Summary:')
        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
        print(f'  Learning Rate: {current_lr:.6f}')

        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch
            torch.save(model.state_dict(), best_model_path)
            print(f'  ✓ New best model saved (Val Acc: {val_acc:.2f}%)')

        print()

    # Load best model for final evaluation
    print(f'\nLoading best model from epoch {best_epoch}...')
    model.load_state_dict(torch.load(best_model_path))

    # Final test evaluation
    print('\nEvaluating on test set...')
    test_loss, test_acc, test_preds, test_labels, test_probs = validate_model(
        model, test_loader, device, phase='Test'
    )

    # Measure inference time
    inf_time = measure_inference_time(model, device)

    # Compile results
    results = {
        'model_name': model_name,
        'history': history,
        'best_epoch': best_epoch,
        'best_val_acc': best_val_acc,
        'test_loss': test_loss,
        'test_acc': test_acc,
        'test_preds': test_preds,
        'test_labels': test_labels,
        'test_probs': test_probs,
        'num_params': num_params,
        'inference_time': inf_time
    }

    print(f'\n{"="*60}')
    print(f'{model_name} Training Complete')
    print(f'{"="*60}')
    print(f'Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})')
    print(f'Test Acc: {test_acc:.2f}%')
    print(f'Parameters: {num_params / 1e6:.2f}M')
    print(f'Inference Time: {inf_time:.2f} ms')
    print(f'{"="*60}\n')

    # Clear GPU memory
    del model
    torch.cuda.empty_cache()

    return results


print('✓ Training loop ready')

"""# Cell 10: Train All Models"""

def train_all_models():
    """Train ConvNeXt, Swin, and Hybrid models"""

    # Model configurations
    models_config = [
        (create_convnext, 'ConvNeXt'),
        (create_swin, 'Swin'),
        (create_hybrid, 'Hybrid')
    ]

    all_results = []

    for model_factory, model_name in models_config:
        results = train_model(model_factory, model_name)
        all_results.append(results)

    return all_results


# Execute training
print('Starting training for all models...\n')
results = train_all_models()

"""# Cell 11: Evaluation and Visualization"""

def evaluate_and_visualize(results):
    """
    Generate comprehensive evaluation metrics and visualizations.

    Args:
        results: List of result dictionaries from training
    """
    print(f'\n{"="*60}')
    print('EVALUATION AND VISUALIZATION')
    print(f'{"="*60}\n')

    # ========================================================================
    # 1. Performance Summary Table
    # ========================================================================
    summary_rows = []

    for res in results:
        name = res['model_name']
        labels = res['test_labels']
        preds = res['test_preds']

        # Calculate metrics
        precision = precision_score(labels, preds, average='macro') * 100
        recall = recall_score(labels, preds, average='macro') * 100
        f1 = f1_score(labels, preds, average='macro') * 100

        summary_rows.append({
            'Model': name,
            'Parameters (M)': res['num_params'] / 1e6,
            'Inference (ms)': res['inference_time'],
            'Test Acc (%)': res['test_acc'],
            'Precision (%)': precision,
            'Recall (%)': recall,
            'F1-Score (%)': f1
        })

    summary_df = pd.DataFrame(summary_rows)

    print('Performance Summary:')
    print(summary_df.to_string(index=False))
    print()

    # Save to CSV
    csv_path = os.path.join(Config.OUTPUT_DIR, 'performance_summary.csv')
    summary_df.to_csv(csv_path, index=False)
    print(f'✓ Summary saved to: {csv_path}\n')

    # ========================================================================
    # 2. Training Curves
    # ========================================================================
    fig, axes = plt.subplots(2, 1, figsize=(10, 10))

    # Loss curves
    for res in results:
        name = res['model_name']
        history = res['history']
        epochs = range(1, len(history['train_loss']) + 1)

        axes[0].plot(epochs, history['train_loss'], label=f'{name} Train', linestyle='-')
        axes[0].plot(epochs, history['val_loss'], label=f'{name} Val', linestyle='--')

    axes[0].set_xlabel('Epoch', fontsize=14)
    axes[0].set_ylabel('Loss', fontsize=14)
    axes[0].set_title('Training and Validation Loss', fontsize=16)
    axes[0].legend(fontsize=12)
    axes[0].tick_params(axis='both', which='major', labelsize=12)
    axes[0].grid(alpha=0.3)

    # Accuracy curves
    for res in results:
        name = res['model_name']
        history = res['history']
        epochs = range(1, len(history['train_acc']) + 1)

        axes[1].plot(epochs, history['train_acc'], label=f'{name} Train', linestyle='-')
        axes[1].plot(epochs, history['val_acc'], label=f'{name} Val', linestyle='--')

    axes[1].set_xlabel('Epoch', fontsize=14)
    axes[1].set_ylabel('Accuracy (%)', fontsize=14)
    axes[1].set_title('Training and Validation Accuracy', fontsize=16)
    axes[1].legend(fontsize=12)
    axes[1].tick_params(axis='both', which='major', labelsize=12)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plot_path = os.path.join(Config.OUTPUT_DIR, 'training_curves.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f'✓ Training curves saved to: {plot_path}\n')

    # ========================================================================
    # 3. Metrics Comparison Bar Charts
    # ========================================================================
    metrics = ['Test Acc (%)', 'Precision (%)', 'Recall (%)', 'F1-Score (%)']
    fig, axes = plt.subplots(2, 2, figsize=(18, 10)) # Adjusted figsize for better layout

    # Flatten the axes array for easier iteration
    axes = axes.flatten()

    for ax, metric in zip(axes, metrics):
        values = summary_df[metric].values
        models = summary_df['Model'].values

        bars = ax.bar(models, values, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
        ax.set_ylabel(metric, fontsize=14)
        ax.set_title(metric, fontsize=16)
        ax.set_ylim([0, 105])
        ax.tick_params(axis='both', which='major', labelsize=12)

        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.2f}',
                   ha='center', va='bottom', fontsize=14)

    plt.tight_layout()
    plot_path = os.path.join(Config.OUTPUT_DIR, 'metrics_comparison.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f'✓ Metrics comparison saved to: {plot_path}\n')

    # ========================================================================
    # 4. Confusion Matrices
    # ========================================================================
    fig, axes = plt.subplots(3, 1, figsize=(8, 18)) # Increased width for labels

    for ax, res in zip(axes, results):
        name = res['model_name']
        labels = res['test_labels']
        preds = res['test_preds']

        cm = confusion_matrix(labels, preds)

        # Normalize to percentages
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

        # Plot
        sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',
                   xticklabels=Config.CLASS_NAMES,
                   yticklabels=Config.CLASS_NAMES,
                   ax=ax, cbar_kws={'label': 'Percentage (%)'}, annot_kws={'size': 14}) # Annotations size
        ax.set_title(f'{name} Confusion Matrix', fontsize=16)
        ax.set_ylabel('True Label', fontsize=14)
        ax.set_xlabel('Predicted Label', fontsize=14)
        ax.tick_params(axis='both', which='major', labelsize=12)

    plt.tight_layout()
    plot_path = os.path.join(Config.OUTPUT_DIR, 'confusion_matrices.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f'✓ Confusion matrices saved to: {plot_path}\n')

    # ========================================================================
    # 5. Per-Class Performance
    # ========================================================================
    fig, axes = plt.subplots(3, 1, figsize=(8, 18)) # Increased width for labels
    metrics_names = ['Precision', 'Recall', 'F1-Score']

    for ax, metric_name in zip(axes, metrics_names):
        class_scores = {cls: [] for cls in Config.CLASS_NAMES}

        for res in results:
            labels = res['test_labels']
            preds = res['test_preds']

            if metric_name == 'Precision':
                scores = precision_score(labels, preds, average=None) * 100
            elif metric_name == 'Recall':
                scores = recall_score(labels, preds, average=None) * 100
            else:  # F1-Score
                scores = f1_score(labels, preds, average=None) * 100

            for idx, cls in enumerate(Config.CLASS_NAMES):
                class_scores[cls].append(scores[idx])

        # Plot grouped bar chart
        x = np.arange(len(Config.CLASS_NAMES))
        width = 0.25

        for i, res in enumerate(results):
            values = [class_scores[cls][i] for cls in Config.CLASS_NAMES]
            ax.bar(x + i * width, values, width, label=res['model_name'])

        ax.set_xlabel('Class', fontsize=14)
        ax.set_ylabel(f'{metric_name} (%)', fontsize=14)
        ax.set_title(f'Per-Class {metric_name}', fontsize=16)
        ax.set_xticks(x + width)
        ax.set_xticklabels(Config.CLASS_NAMES, rotation=45, ha='right', fontsize=12)
        ax.legend(fontsize=12)
        ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plot_path = os.path.join(Config.OUTPUT_DIR, 'per_class_performance.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f'✓ Per-class performance saved to: {plot_path}\n')

    # ========================================================================
    # 6. Detailed Classification Reports
    # ========================================================================
    for res in results:
        name = res['model_name']
        labels = res['test_labels']
        preds = res['test_preds']

        print(f'\n{"="*60}')
        print(f'Classification Report - {name}')
        print(f'{"="*60}')

        report = classification_report(
            labels, preds,
            target_names=Config.CLASS_NAMES,
            digits=4
        )
        print(report)

        # Save report to file
        report_path = os.path.join(Config.OUTPUT_DIR, f'{name}_classification_report.txt')
        with open(report_path, 'w') as f:
            f.write(f'Classification Report - {name}\n')
            f.write('='*60 + '\n')
            f.write(report)

        print(f'✓ Report saved to: {report_path}')

    # ========================================================================
    # 7. Model Efficiency Analysis
    # ========================================================================
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    models = summary_df['Model'].values
    params = summary_df['Parameters (M)'].values
    inf_time = summary_df['Inference (ms)'].values
    accuracy = summary_df['Test Acc (%)'].values

    # Parameters vs Accuracy
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    for i, (m, p, a) in enumerate(zip(models, params, accuracy)):
        ax1.scatter(p, a, s=200, c=colors[i], label=m, alpha=0.7)
    ax1.set_xlabel('Parameters (Millions)', fontsize=14)
    ax1.set_ylabel('Test Accuracy (%)', fontsize=14)
    ax1.set_title('Model Size vs Accuracy', fontsize=16)
    ax1.legend(fontsize=12)
    ax1.tick_params(axis='both', which='major', labelsize=12)
    ax1.grid(alpha=0.3)

    # Inference Time vs Accuracy
    for i, (m, t, a) in enumerate(zip(models, inf_time, accuracy)):
        ax2.scatter(t, a, s=200, c=colors[i], label=m, alpha=0.7)
    ax2.set_xlabel('Inference Time (ms)', fontsize=14)
    ax2.set_ylabel('Test Accuracy (%)', fontsize=14)
    ax2.set_title('Inference Speed vs Accuracy', fontsize=16)
    ax2.legend(fontsize=12)
    ax2.tick_params(axis='both', which='major', labelsize=12)
    ax2.grid(alpha=0.3)

    plt.tight_layout()
    plot_path = os.path.join(Config.OUTPUT_DIR, 'efficiency_analysis.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f'\n✓ Efficiency analysis saved to: {plot_path}\n')

    # ========================================================================
    # 8. Save Complete Results
    # ========================================================================
    results_path = os.path.join(Config.OUTPUT_DIR, 'complete_results.json')

    # Prepare JSON-serializable results
    json_results = []
    for res in results:
        json_res = {
            'model_name': res['model_name'],
            'best_epoch': int(res['best_epoch']),
            'best_val_acc': float(res['best_val_acc']),
            'test_acc': float(res['test_acc']),
            'test_loss': float(res['test_loss']),
            'num_params': int(res['num_params']),
            'inference_time': float(res['inference_time']),
            'history': {
                'train_loss': [float(x) for x in res['history']['train_loss']],
                'train_acc': [float(x) for x in res['history']['train_acc']],
                'val_loss': [float(x) for x in res['history']['val_loss']],
                'val_acc': [float(x) for x in res['history']['val_acc']],
                'learning_rates': [float(x) for x in res['history']['learning_rates']]
            }
        }
        json_results.append(json_res)

    with open(results_path, 'w') as f:
        json.dump(json_results, f, indent=4)

    print(f'✓ Complete results saved to: {results_path}\n')

    print(f'{"="*60}')
    print('EVALUATION COMPLETE')
    print(f'{"="*60}')
    print(f'All results saved to: {Config.OUTPUT_DIR}')
    print(f'{"="*60}\n')


# Run evaluation and visualization
evaluate_and_visualize(results)

"""# Cell 12: Final Summary and Recommendations"""

def print_final_summary(results):
    """Print final summary and recommendations"""

    print(f'\n{"="*70}')
    print('FINAL SUMMARY AND RECOMMENDATIONS')
    print(f'{"="*70}\n')

    # Find best model
    best_model = max(results, key=lambda x: x['test_acc'])

    print('Best Performing Model:')
    print(f'  Model: {best_model["model_name"]}')
    print(f'  Test Accuracy: {best_model["test_acc"]:.2f}%')
    print(f'  Parameters: {best_model["num_params"] / 1e6:.2f}M')
    print(f'  Inference Time: {best_model["inference_time"]:.2f} ms')
    print()

    # Recommendations
    print('Recommendations:')
    print()

    # Most accurate
    most_accurate = max(results, key=lambda x: x['test_acc'])
    print(f'1. For Maximum Accuracy:')
    print(f'   → Use {most_accurate["model_name"]} ({most_accurate["test_acc"]:.2f}%)')
    print()

    # Most efficient
    fastest = min(results, key=lambda x: x['inference_time'])
    print(f'2. For Fastest Inference:')
    print(f'   → Use {fastest["model_name"]} ({fastest["inference_time"]:.2f} ms)')
    print()

    # Best trade-off
    print(f'3. Best Accuracy/Speed Trade-off:')
    tradeoff_scores = []
    for res in results:
        # Normalize metrics and compute balanced score
        acc_norm = res['test_acc'] / 100
        time_norm = 1 / (1 + res['inference_time'] / 10)
        score = (acc_norm + time_norm) / 2
        tradeoff_scores.append((res['model_name'], score, res['test_acc'], res['inference_time']))

    best_tradeoff = max(tradeoff_scores, key=lambda x: x[1])
    print(f'   → Use {best_tradeoff[0]} (Acc: {best_tradeoff[2]:.2f}%, Time: {best_tradeoff[3]:.2f} ms)')
    print()

    # Clinical deployment considerations
    # print('Clinical Deployment Considerations:')
    # print('  • All models achieve >95% accuracy for reliable clinical use')
    # print('  • Hybrid model provides best feature fusion but requires more compute')
    # print('  • ConvNeXt offers good balance of accuracy and efficiency')
    # print('  • Swin Transformer excels at capturing global context')
    # print()

    # print('Next Steps:')
    # print('  1. Fine-tune the best model with additional epochs')
    # print('  2. Perform cross-validation for robust performance estimates')
    # print('  3. Test on external datasets for generalization')
    # print('  4. Implement ensemble methods combining all three models')
    # print('  5. Deploy using ONNX/TensorRT for production optimization')

    print(f'\n{"="*70}')
    print('ALL TRAINING AND EVALUATION COMPLETE')
    print(f'{"="*70}\n')


# Print final summary
print_final_summary(results)

"""# Final: Load and Test a Saved Model"""

def load_and_test_model(model_name):
    """
    Example function to load a saved model and make predictions.

    Args:
        model_name: Name of the model ('ConvNeXt', 'Swin', 'Hybrid')
    """
    print(f'\nLoading saved {model_name} model...')

    # Create model
    if model_name == 'ConvNeXt':
        model = create_convnext()
    elif model_name == 'Swin':
        model = create_swin()
    elif model_name == 'Hybrid':
        model = create_hybrid()
    else:
        raise ValueError(f'Unknown model: {model_name}')

    # Load weights
    model_path = os.path.join(Config.OUTPUT_DIR, f'{model_name}_best.pth')
    model.load_state_dict(torch.load(model_path))
    model = model.to(device)
    model.eval()

    print(f'✓ Model loaded from: {model_path}')

    return model

# ========================================================================
# ---> Testing Goes Here
# ========================================================================

# Example: Load best model for inference
# best_model_name = max(results, key=lambda x: x['test_acc'])['model_name']
# loaded_model = load_and_test_model(best_model_name)


print('\n' + '='*70)
print('NOTEBOOK EXECUTION COMPLETE')
print('='*70)
print(f'Results directory: {Config.OUTPUT_DIR}')
print('All models trained, evaluated, and saved successfully!')
print('='*70)